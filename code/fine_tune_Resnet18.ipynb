{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fede505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef2d653",
   "metadata": {},
   "source": [
    "train 1210\n",
    "test 302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c57540cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msba7028gpu\\anaconda3\\envs\\d2l\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\msba7028gpu\\anaconda3\\envs\\d2l\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27761e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(100)\n",
    "new_data_dir = r\"C:\\Users\\msba7028gpu\\Desktop\\train_valid_test\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22b1449",
   "metadata": {},
   "source": [
    "# image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f870e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(100)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.08, 1.0),  \n",
    "                                 ratio=(3.0/4.0, 4.0/3.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # randomly change brightness, contrast, saturation and hue\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    #transforms.Resize(256),\n",
    "    #transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf535a",
   "metadata": {},
   "source": [
    "# data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7603122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = torchvision.datasets.ImageFolder(root=os.path.join(new_data_dir, 'train'),\n",
    "                                            transform=transform_train)\n",
    "test_ds = torchvision.datasets.ImageFolder(root=os.path.join(new_data_dir, 'test'),\n",
    "                                            transform=transform_test)\n",
    "batch_size = 32   #batch size can change\n",
    "train_iter = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)  # shuffle=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8f62d",
   "metadata": {},
   "source": [
    "# k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5fc2f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#dataset = ConcatDataset([train_ds, test_ds])\n",
    "dataset = train_ds\n",
    "\n",
    "num_epochs=60\n",
    "batch_size=32\n",
    "#k=5\n",
    "#splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "#foldperf={}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35dc8eb",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67028e55-bff4-4577-8d11-051ec5f581be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net():\n",
    "    net = torchvision.models.resnet18(pretrained=True)\n",
    "    net.fc = nn.Linear(net.fc.in_features, 8)\n",
    "    nn.init.xavier_uniform_(net.fc.weight)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a580f892-86bd-4c7b-beb4-592b27d3a1f6",
   "metadata": {},
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddc1849b-2949-4912-9652-61409c2b646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch_ch13(net, X, y, loss, trainer, devices):\n",
    "    \"\"\"Train for a minibatch with mutiple GPUs (defined in Chapter 13).\"\"\"\n",
    "    if isinstance(X, list):\n",
    "        \n",
    "        X = [x.to(devices[0]) for x in X]\n",
    "    else:\n",
    "        X = X.to(devices[0])\n",
    "    y = y.to(devices[0])\n",
    "    net.train()\n",
    "    trainer.zero_grad()\n",
    "    pred = net(X)\n",
    "    l = loss(pred, y)\n",
    "    l.sum().backward()\n",
    "    trainer.step()\n",
    "    train_loss_sum = l.sum()\n",
    "    train_acc_sum = d2l.accuracy(pred, y)\n",
    "    return train_loss_sum, train_acc_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3c63894-7705-4639-980b-f2cc2592d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "def train_model(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "               devices=d2l.try_all_gpus()):\n",
    "    \"\"\"Train a model with mutiple GPUs (defined in Chapter 13).\"\"\"\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Sum of training loss, sum of training accuracy, no. of examples,\n",
    "        # no. of predictions\n",
    "        metric = d2l.Accumulator(4)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            l, acc = train_batch_ch13(\n",
    "                net, features, labels, loss, trainer, devices)\n",
    "            metric.add(l, acc, labels.shape[0], labels.numel())\n",
    "            timer.stop()\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "    \n",
    "    loss = metric[0] / metric[2]\n",
    "    train_acc = metric[1] / metric[3]\n",
    "\n",
    "    return loss,train_acc,test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ceb3e5b-fdc7-4d2d-baf3-43afbb5fe409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fine_tuning(net, learning_rate, trainloader, testloader, batch_size, num_epochs,\n",
    "                      param_group=True):\n",
    "    devices = d2l.try_all_gpus()\n",
    "    loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    if param_group:\n",
    "        params_1x = [param for name, param in net.named_parameters()\n",
    "             if name not in [\"fc.weight\", \"fc.bias\"]]\n",
    "        trainer = torch.optim.SGD([{'params': params_1x},\n",
    "                                   {'params': net.fc.parameters(),\n",
    "                                    'lr': learning_rate * 10}],\n",
    "                                lr=learning_rate, weight_decay=0.01)\n",
    "    else:\n",
    "        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,\n",
    "                                  weight_decay=0.01)\n",
    "    \n",
    "    loss,train_acc,test_acc = train_model(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "               devices=d2l.try_all_gpus())\n",
    "    return loss,train_acc,test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dac876cf-8906-4950-a5f4-0288a8f8a803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_result(learning_rate,batch_size,num_epochs,param_group=True):\n",
    "    loss_ls =[]\n",
    "    train_acc_ls =[]\n",
    "    test_acc_ls = []\n",
    "    for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "\n",
    "        print('Fold {}'.format(fold + 1))\n",
    "\n",
    "        #all test there means validation\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_idx) \n",
    "        test_sampler = SubsetRandomSampler(val_idx)\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "        \n",
    "        net = get_net()\n",
    "\n",
    "        loss,train_acc,test_acc = train_fine_tuning(net, learning_rate, train_loader, test_loader, batch_size, num_epochs,\n",
    "                          param_group=True)\n",
    "\n",
    "        loss_ls.append(loss)\n",
    "        train_acc_ls.append(train_acc)\n",
    "        test_acc_ls.append(test_acc)\n",
    "        print('loss: ',loss)\n",
    "        print('train: ',train_acc)\n",
    "        print('test: ',test_acc)\n",
    "    return loss_ls,train_acc_ls,test_acc_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "350e3b1c-987b-4a74-afbd-a15e21c74ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "loss:  0.6228450002749104\n",
      "train:  0.7834710743801653\n",
      "test:  0.7417218543046358\n",
      "Fold 2\n",
      "loss:  0.5865348815917969\n",
      "train:  0.8041322314049587\n",
      "test:  0.7814569536423841\n",
      "Fold 3\n",
      "loss:  0.5925184155298658\n",
      "train:  0.8066115702479338\n",
      "test:  0.7615894039735099\n",
      "Fold 4\n",
      "loss:  0.6166397591267736\n",
      "train:  0.7776859504132232\n",
      "test:  0.7880794701986755\n",
      "Fold 5\n",
      "loss:  0.5718248004755698\n",
      "train:  0.8049586776859504\n",
      "test:  0.7947019867549668\n"
     ]
    }
   ],
   "source": [
    "loss_ls,train_acc_ls,test_acc_ls = cv_result(learning_rate = 5e-5,batch_size=32,num_epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32c0a8be-268b-4331-b2df-08d5736aa871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5fold_average_loss: 0.598, 5fold_average_train_accuracy: 0.795, 5fold_average_test_accuracy: 0.774\n"
     ]
    }
   ],
   "source": [
    "loss_avg = sum(loss_ls)/len(loss_ls)\n",
    "train_acc_avg = sum(train_acc_ls)/len(train_acc_ls)\n",
    "test_acc_avg = sum(test_acc_ls)/len(test_acc_ls)\n",
    "print(f'5fold_average_loss: {loss_avg:.3f}, 5fold_average_train_accuracy: '\n",
    "      f'{train_acc_avg:.3f}, 5fold_average_test_accuracy: {test_acc_avg:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0beb45b-8fbb-4eff-9f3f-f94e0413961a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "loss:  0.3773840387990652\n",
      "train:  0.8785123966942149\n",
      "test:  0.7947019867549668\n",
      "Fold 2\n",
      "loss:  0.38814979131556737\n",
      "train:  0.868595041322314\n",
      "test:  0.8145695364238411\n",
      "Fold 3\n",
      "loss:  0.3973514454423889\n",
      "train:  0.8677685950413223\n",
      "test:  0.7947019867549668\n",
      "Fold 4\n",
      "loss:  0.38299530951444766\n",
      "train:  0.8743801652892562\n",
      "test:  0.804635761589404\n",
      "Fold 5\n",
      "loss:  0.3695365759952009\n",
      "train:  0.8793388429752066\n",
      "test:  0.7980132450331126\n"
     ]
    }
   ],
   "source": [
    "loss_ls,train_acc_ls,test_acc_ls = cv_result(learning_rate = 10e-5,batch_size=32,num_epochs = 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1a50317-280e-48ca-9024-7afea294890e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5fold_average_loss: 0.383, 5fold_average_train_accuracy: 0.874, 5fold_average_test_accuracy: 0.801\n"
     ]
    }
   ],
   "source": [
    "loss_avg = sum(loss_ls)/len(loss_ls)\n",
    "train_acc_avg = sum(train_acc_ls)/len(train_acc_ls)\n",
    "test_acc_avg = sum(test_acc_ls)/len(test_acc_ls)\n",
    "print(f'5fold_average_loss: {loss_avg:.3f}, 5fold_average_train_accuracy: '\n",
    "      f'{train_acc_avg:.3f}, 5fold_average_test_accuracy: {test_acc_avg:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "812d4442-a2dd-438e-8364-94e9d246f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fine_tuning(net, learning_rate, trainloader, testloader, batch_size, num_epochs,\n",
    "                      param_group=True):\n",
    "    devices = d2l.try_all_gpus()\n",
    "    loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    if param_group:\n",
    "        params_1x = [param for name, param in net.named_parameters()\n",
    "             if name not in [\"fc.weight\", \"fc.bias\"]]\n",
    "        trainer = torch.optim.SGD([{'params': params_1x},\n",
    "                                   {'params': net.fc.parameters(),\n",
    "                                    'lr': learning_rate * 10}],\n",
    "                                lr=learning_rate, weight_decay=0.001)\n",
    "    else:\n",
    "        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,\n",
    "                                  weight_decay=0.001)\n",
    "    \n",
    "    loss,train_acc,test_acc = train_model(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "               devices=d2l.try_all_gpus())\n",
    "    return loss,train_acc,test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4718bf78-38d4-46b3-a160-1feba3c038a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "loss:  0.4386138025394156\n",
      "train:  0.8504132231404958\n",
      "test:  0.7913907284768212\n",
      "Fold 2\n",
      "loss:  0.5058183094686713\n",
      "train:  0.8264462809917356\n",
      "test:  0.7847682119205298\n",
      "Fold 3\n",
      "loss:  0.4878374127317066\n",
      "train:  0.8404958677685951\n",
      "test:  0.8211920529801324\n",
      "Fold 4\n",
      "loss:  0.43650174298562294\n",
      "train:  0.8487603305785124\n",
      "test:  0.8211920529801324\n",
      "Fold 5\n",
      "loss:  0.48207707129234123\n",
      "train:  0.8314049586776859\n",
      "test:  0.7880794701986755\n"
     ]
    }
   ],
   "source": [
    "loss_ls,train_acc_ls,test_acc_ls = cv_result(learning_rate = 10e-5,batch_size=32,num_epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81f68898-2925-4bc7-a4a6-b7a23217d8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5fold_average_loss: 0.470, 5fold_average_train_accuracy: 0.840, 5fold_average_test_accuracy: 0.801\n"
     ]
    }
   ],
   "source": [
    "loss_avg = sum(loss_ls)/len(loss_ls)\n",
    "train_acc_avg = sum(train_acc_ls)/len(train_acc_ls)\n",
    "test_acc_avg = sum(test_acc_ls)/len(test_acc_ls)\n",
    "print(f'5fold_average_loss: {loss_avg:.3f}, 5fold_average_train_accuracy: '\n",
    "      f'{train_acc_avg:.3f}, 5fold_average_test_accuracy: {test_acc_avg:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12752df6-61aa-43cd-9ab4-af8a42e6fb05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
