{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fede505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd418a",
   "metadata": {},
   "source": [
    "# data split(train+test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29299ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['382.jpeg', '339.jpeg', '1366.jpeg', '742.jpeg', '1327.jpeg', '1421.jpeg', '1455.jpeg', '1242.jpeg', '1157.jpeg', '1460.jpeg', '982.jpeg', '343.jpeg', '616.jpeg', '9.jpeg', '929.jpeg', '630.jpeg', '1162.jpeg', '788.jpeg', '54.jpeg', '1365.jpeg', '414.jpeg', '97.jpeg', '768.jpeg', '722.jpeg', '1397.jpeg', '326.jpeg', '352.jpeg', '896.jpeg', '139.jpeg', '1278.jpeg', '489.jpeg', '855.jpeg', '683.jpeg', '261.jpeg', '115.jpeg', '959.jpeg', '611.jpeg', '887.jpeg', '1159.jpeg', '501.jpeg', '22.jpeg', '38.jpeg', '1076.jpeg', '1494.jpeg', '666.jpeg', '1010.jpeg', '420.jpeg', '1373.jpeg', '305.jpeg', '622.jpeg', '1146.jpeg', '49.jpeg', '594.jpeg', '1362.jpeg', '754.jpeg', '490.jpeg', '1092.jpeg', '297.jpeg', '1235.jpeg', '1396.jpeg', '956.jpeg', '1294.jpeg', '451.jpeg', '735.jpeg', '1324.jpeg', '565.jpeg', '1321.jpeg', '1065.jpeg', '12.jpeg', '104.jpeg', '1312.jpeg', '510.jpeg', '899.jpeg', '41.jpeg', '1147.jpeg', '421.jpeg', '802.jpeg', '1156.jpeg', '1260.jpeg', '208.jpeg', '776.jpeg', '1007.jpeg', '34.jpeg', '245.jpeg', '1248.jpeg', '601.jpeg', '385.jpeg', '1368.jpeg', '11.jpeg', '818.jpeg', '1388.jpeg', '454.jpeg', '1472.jpeg', '643.jpeg', '123.jpeg', '29.jpeg', '919.jpeg', '515.jpeg', '110.jpeg', '1239.jpeg', '962.jpeg', '743.jpeg', '1275.jpeg', '759.jpeg', '390.jpeg', '1226.jpeg', '1175.jpeg', '1216.jpeg', '1262.jpeg', '183.jpeg', '1318.jpeg', '393.jpeg', '1266.jpeg', '1380.jpeg', '671.jpeg', '225.jpeg', '558.jpeg', '1429.jpeg', '1213.jpeg', '303.jpeg', '134.jpeg', '889.jpeg', '1130.jpeg', '561.jpeg', '168.jpeg', '1017.jpeg', '717.jpeg', '1085.jpeg', '26.jpeg', '332.jpeg', '373.jpeg', '258.jpeg', '349.jpeg', '400.jpeg', '320.jpeg', '1168.jpeg', '1331.jpeg', '1066.jpeg', '324.jpeg', '968.jpeg', '372.jpeg', '456.jpeg', '210.jpeg', '844.jpeg', '1413.jpeg', '842.jpeg', '965.jpeg', '21.jpeg', '1192.jpeg', '905.jpeg', '967.jpeg', '215.jpeg', '140.jpeg', '95.jpeg', '880.jpeg', '1458.jpeg', '344.jpeg', '445.jpeg', '1145.jpeg', '444.jpeg', '413.jpeg', '147.jpeg', '1125.jpeg', '178.jpeg', '90.jpeg', '228.jpeg', '517.jpeg', '620.jpeg', '1231.jpeg', '1470.jpeg', '431.jpeg', '173.jpeg', '392.jpeg', '851.jpeg', '764.jpeg', '1291.jpeg', '158.jpeg', '336.jpeg', '751.jpeg', '779.jpeg', '867.jpeg', '955.jpeg', '449.jpeg', '1356.jpeg', '337.jpeg', '1361.jpeg', '274.jpeg', '1004.jpeg', '868.jpeg', '292.jpeg', '918.jpeg', '181.jpeg', '1353.jpeg', '863.jpeg', '869.jpeg', '1313.jpeg', '817.jpeg', '1479.jpeg', '849.jpeg', '405.jpeg', '94.jpeg', '1423.jpeg', '848.jpeg', '374.jpeg', '1003.jpeg', '1204.jpeg', '562.jpeg', '618.jpeg', '1370.jpeg', '10.jpeg', '60.jpeg', '209.jpeg', '877.jpeg', '1305.jpeg', '58.jpeg', '1376.jpeg', '1484.jpeg', '50.jpeg', '670.jpeg', '792.jpeg', '780.jpeg', '801.jpeg', '167.jpeg', '1367.jpeg', '1035.jpeg', '1191.jpeg', '203.jpeg', '1136.jpeg', '707.jpeg', '1056.jpeg', '1104.jpeg', '19.jpeg', '318.jpeg', '665.jpeg', '948.jpeg', '721.jpeg', '698.jpeg', '1355.jpeg', '1182.jpeg', '365.jpeg', '1314.jpeg', '1486.jpeg', '487.jpeg', '243.jpeg', '1234.jpeg', '1238.jpeg', '460.jpeg', '931.jpeg', '354.jpeg', '408.jpeg', '673.jpeg', '30.jpeg', '1459.jpeg', '1501.jpeg', '491.jpeg', '693.jpeg', '928.jpeg', '342.jpeg', '1336.jpeg', '520.jpeg', '694.jpeg', '791.jpeg', '131.jpeg', '1135.jpeg', '617.jpeg', '1439.jpeg', '485.jpeg', '1025.jpeg', '1209.jpeg', '548.jpeg', '113.jpeg', '256.jpeg', '845.jpeg', '990.jpeg', '688.jpeg', '892.jpeg', '62.jpeg', '1504.jpeg', '805.jpeg', '1019.jpeg', '711.jpeg', '40.jpeg', '148.jpeg', '1302.jpeg', '37.jpeg', '1064.jpeg', '269.jpeg', '580.jpeg', '1420.jpeg', '661.jpeg', '176.jpeg', '129.jpeg', '447.jpeg', '1102.jpeg', '92.jpeg', '407.jpeg', '1345.jpeg', '1212.jpeg', '530.jpeg', '150.jpeg', '1505.jpeg', '31.jpeg']\n"
     ]
    }
   ],
   "source": [
    "random.seed(99)\n",
    "\n",
    "def moveFile(train_img_Dir):\n",
    "        img_pathDir = os.listdir(train_img_Dir)                    # Extract the original path of the image\n",
    "        filenumber = len(img_pathDir)\n",
    "        test_rate = 0.2                                            # 0.2，means 20% of whole dataset is test\n",
    "        test_picknumber = int(filenumber*test_rate)                # Take a certain number of pictures from the folder according to the test_rate ratio\n",
    "        # Select samples to move to test\n",
    "        sample1 = random.sample(img_pathDir, test_picknumber)      # Randomly select sample images of the number of picknumbers\n",
    "        print(sample1)\n",
    "        for i in range(0, len(sample1)):\n",
    "            sample1[i] = sample1[i][:-5]                           # Remove the extension of the image, this list is required when moving the annotation\n",
    "        for name in sample1:\n",
    "            src_img_name1 = train_img_Dir + name\n",
    "            dst_img_name1 = test_img_Dir + name\n",
    "            shutil.move(src_img_name1 + \".jpeg\", dst_img_name1 + \".jpeg\")     # Add the extension of the image to move the image\n",
    "        return\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # train moves from train\n",
    "    train_img_Dir = \"../Car Damage Images and Index/train/\"\n",
    "    \n",
    "    # test path: image and annotation directory\n",
    "    test_img_Dir = \"../Car Damage Images and Index/test/\"\n",
    "    if not os.path.exists(test_img_Dir):\n",
    "        os.makedirs(test_img_Dir)\n",
    "    \n",
    "    # Run the Divide Dataset function\n",
    "    moveFile(train_img_Dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef2d653",
   "metadata": {},
   "source": [
    "train 1210\n",
    "test 302"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06c73e8",
   "metadata": {},
   "source": [
    "# 创建data标准存放格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c57540cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "# set random seed\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27761e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(100)\n",
    "data_dir = '../Car Damage Images and Index/'  \n",
    "label_file, train_dir, test_dir = 'data.xlsx', 'train', 'test'  \n",
    "new_data_dir = './train_valid_test'  # The directory where the sorted data is stored\n",
    "def mkdir_if_not_exist(path):\n",
    "    # If the directory path does not exist, create the directory\n",
    "    if not os.path.exists(os.path.join(*path)):\n",
    "        os.makedirs(os.path.join(*path))\n",
    "        \n",
    "def reorg_dog_data(data_dir, label_file, train_dir, test_dir, new_data_dir):\n",
    "    # Read training data labels\n",
    "    labels = pd.read_excel(os.path.join(data_dir, label_file))\n",
    "    #labels = labels.drop(columns='image')\n",
    "    id2label = {ID: label for ID, label in labels.values}  # (key: value): (id: label)\n",
    "\n",
    "    # Shuffle the training data randomly\n",
    "    train_files = os.listdir(os.path.join(data_dir, train_dir))\n",
    "    random.shuffle(train_files)    \n",
    "\n",
    "    # train dataset\n",
    "    train_size = int(len(train_files))  # trainset size\n",
    "    for i, file in enumerate(train_files):\n",
    "        img_id = file.split('.')[0]  # file  id.jpg\n",
    "        img_label = id2label.get(int(img_id))\n",
    "        if i < train_size:\n",
    "            mkdir_if_not_exist([new_data_dir, 'train', img_label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, file),\n",
    "                        os.path.join(new_data_dir, 'train', img_label))\n",
    "\n",
    "    # test dataset\n",
    "    test_files = os.listdir(os.path.join(data_dir, test_dir))\n",
    "    test_size = len(test_files)\n",
    "    for i, file in enumerate(test_files):\n",
    "        img_id = file.split('.')[0]\n",
    "        img_label = id2label.get(int(img_id))\n",
    "        if i < test_size:\n",
    "            mkdir_if_not_exist([new_data_dir, 'test', img_label])\n",
    "            shutil.copy(os.path.join(data_dir, test_dir, file),\n",
    "                        os.path.join(new_data_dir, 'test', img_label))\n",
    "random.seed(100)\n",
    "reorg_dog_data(data_dir, label_file, train_dir, test_dir, new_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22b1449",
   "metadata": {},
   "source": [
    "# image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f870e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(100)\n",
    "transform_train = transforms.Compose([\n",
    "    # Randomly crop the image to an area of 0.08~1 times the original image area, and the ratio of height and width is 3/4~4/3, and then scale it to a new image with height and width of 224 pixels\n",
    "    transforms.RandomResizedCrop(224, scale=(0.08, 1.0),  \n",
    "                                 ratio=(3.0/4.0, 4.0/3.0)),\n",
    "    # Random horizontal flip with probability 0.5\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # randomly change brightness, contrast, saturation and hue\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "# Image enhancement on the test set only does deterministic operations\n",
    "transform_test = transforms.Compose([\n",
    "    #transforms.Resize(256),\n",
    "    #transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf535a",
   "metadata": {},
   "source": [
    "# data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7603122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_dir = './train_valid_test'\n",
    "train_ds = torchvision.datasets.ImageFolder(root=os.path.join(new_data_dir, 'train'),\n",
    "                                            transform=transform_train)\n",
    "test_ds = torchvision.datasets.ImageFolder(root=os.path.join(new_data_dir, 'test'),\n",
    "                                            transform=transform_test)\n",
    "batch_size = 32   #batch size can change\n",
    "train_iter = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)  # shuffle=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f2414e",
   "metadata": {},
   "source": [
    "# k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1200e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "dataset = train_ds\n",
    "#dataset = ConcatDataset([train_ds, test_ds])\n",
    "\n",
    "#num_epochs=60\n",
    "batch_size=32\n",
    "#k=5\n",
    "#splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "#foldperf={}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2de0076",
   "metadata": {},
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e188d2",
   "metadata": {},
   "source": [
    "for finetuning: param_group=True\n",
    "\n",
    "\n",
    "for sratch net: param_group=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "675255a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch_ch13(net, X, y, loss, trainer, devices):\n",
    "    \"\"\"Train for a minibatch with mutiple GPUs (defined in Chapter 13).\"\"\"\n",
    "    if isinstance(X, list):\n",
    "        # Required for BERT fine-tuning (to be covered later)\n",
    "        X = [x.to(devices[0]) for x in X]\n",
    "    else:\n",
    "        X = X.to(devices[0])\n",
    "    y = y.to(devices[0])\n",
    "    net.train()\n",
    "    trainer.zero_grad()\n",
    "    pred = net(X)\n",
    "    l = loss(pred, y)\n",
    "    l.sum().backward()\n",
    "    trainer.step()\n",
    "    train_loss_sum = l.sum()\n",
    "    train_acc_sum = d2l.accuracy(pred, y)\n",
    "    return train_loss_sum, train_acc_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51e6b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "def train_model(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "               devices=d2l.try_all_gpus()):\n",
    "    \"\"\"Train a model with mutiple GPUs (defined in Chapter 13).\"\"\"\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Sum of training loss, sum of training accuracy, no. of examples,\n",
    "        # no. of predictions\n",
    "        metric = d2l.Accumulator(4)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            l, acc = train_batch_ch13(\n",
    "                net, features, labels, loss, trainer, devices)\n",
    "            metric.add(l, acc, labels.shape[0], labels.numel())\n",
    "            timer.stop()\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "    \n",
    "    loss = metric[0] / metric[2]\n",
    "    train_acc = metric[1] / metric[3]\n",
    "\n",
    "    return loss,train_acc,test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c750cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_result(learning_rate,batch_size,num_epochs,param_group=True):\n",
    "    loss_ls =[]\n",
    "    train_acc_ls =[]\n",
    "    test_acc_ls = []\n",
    "    for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "\n",
    "        print('Fold {}'.format(fold + 1))\n",
    "\n",
    "        #all test there means validation\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_idx) \n",
    "        test_sampler = SubsetRandomSampler(val_idx)\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "        \n",
    "        net = get_net()\n",
    "\n",
    "        loss,train_acc,test_acc = train_fine_tuning(net, learning_rate, train_loader, test_loader, batch_size, num_epochs,\n",
    "                          param_group=True)\n",
    "\n",
    "        loss_ls.append(loss)\n",
    "        train_acc_ls.append(train_acc)\n",
    "        test_acc_ls.append(test_acc)\n",
    "        print('loss: ',loss)\n",
    "        print('train: ',train_acc)\n",
    "        print('test: ',test_acc)\n",
    "    return loss_ls,train_acc_ls,test_acc_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb3f577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fine_tuning(net, learning_rate, trainloader, testloader, batch_size=128, num_epochs=20,\n",
    "                      param_group=True):\n",
    "    devices = d2l.try_all_gpus()\n",
    "    loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    if param_group:\n",
    "        params_1x = [param for name, param in net.named_parameters()\n",
    "             if name not in [\"fc.weight\", \"fc.bias\"]]\n",
    "        trainer = torch.optim.SGD([{'params': params_1x},\n",
    "                                   {'params': net.fc.parameters(),\n",
    "                                    'lr': learning_rate * 10}],\n",
    "                                lr=learning_rate, weight_decay=0.001)\n",
    "    else:\n",
    "        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,\n",
    "                                  weight_decay=0.001)\n",
    "    \n",
    "    loss,train_acc,test_acc = train_model(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "               devices=d2l.try_all_gpus())\n",
    "    return loss,train_acc,test_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
