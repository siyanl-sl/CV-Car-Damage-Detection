{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fede505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef2d653",
   "metadata": {},
   "source": [
    "train 1210\n",
    "test 302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c57540cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msba7028gpu\\anaconda3\\envs\\d2l\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\msba7028gpu\\anaconda3\\envs\\d2l\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27761e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(100)\n",
    "new_data_dir = r\"C:\\Users\\msba7028gpu\\Desktop\\train_valid_test\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22b1449",
   "metadata": {},
   "source": [
    "# image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f870e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(100)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.08, 1.0),  \n",
    "                                 ratio=(3.0/4.0, 4.0/3.0)),\n",
    "\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # randomly change brightness, contrast, saturation and hue\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    #transforms.Resize(256),\n",
    "    #transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf535a",
   "metadata": {},
   "source": [
    "# data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7603122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = torchvision.datasets.ImageFolder(root=os.path.join(new_data_dir, 'train'),\n",
    "                                            transform=transform_train)\n",
    "test_ds = torchvision.datasets.ImageFolder(root=os.path.join(new_data_dir, 'test'),\n",
    "                                            transform=transform_test)\n",
    "batch_size = 32   #batch size can change\n",
    "train_iter = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)  # shuffle=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8f62d",
   "metadata": {},
   "source": [
    "# k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5fc2f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#dataset = ConcatDataset([train_ds, test_ds])\n",
    "dataset = train_ds\n",
    "\n",
    "num_epochs=60\n",
    "batch_size=32\n",
    "#k=5\n",
    "#splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "#foldperf={}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35dc8eb",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67028e55-bff4-4577-8d11-051ec5f581be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net():\n",
    "    net = torchvision.models.vgg19(pretrained=True)\n",
    "    net.classifier[-1] = nn.Linear(in_features=4096, out_features=8)\n",
    "    nn.init.xavier_uniform_(net.classifier[-1].weight)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de2194-53cd-4e06-b30a-a287cc9138c6",
   "metadata": {},
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9591669-95b7-4584-aac6-9bdfc8595c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch_ch13(net, X, y, loss, trainer, devices):\n",
    "    \"\"\"Train for a minibatch with mutiple GPUs (defined in Chapter 13).\"\"\"\n",
    "    if isinstance(X, list):\n",
    "        # Required for BERT fine-tuning (to be covered later)\n",
    "        X = [x.to(devices[0]) for x in X]\n",
    "    else:\n",
    "        X = X.to(devices[0])\n",
    "    y = y.to(devices[0])\n",
    "    net.train()\n",
    "    trainer.zero_grad()\n",
    "    pred = net(X)\n",
    "    l = loss(pred, y)\n",
    "    l.sum().backward()\n",
    "    trainer.step()\n",
    "    train_loss_sum = l.sum()\n",
    "    train_acc_sum = d2l.accuracy(pred, y)\n",
    "    return train_loss_sum, train_acc_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aedd38d3-fff8-4cd1-bb93-bcf08c63d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "def train_model(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "               devices=d2l.try_all_gpus()):\n",
    "    \"\"\"Train a model with mutiple GPUs (defined in Chapter 13).\"\"\"\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Sum of training loss, sum of training accuracy, no. of examples,\n",
    "        # no. of predictions\n",
    "        metric = d2l.Accumulator(4)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            l, acc = train_batch_ch13(\n",
    "                net, features, labels, loss, trainer, devices)\n",
    "            metric.add(l, acc, labels.shape[0], labels.numel())\n",
    "            timer.stop()\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "    \n",
    "    loss = metric[0] / metric[2]\n",
    "    train_acc = metric[1] / metric[3]\n",
    "\n",
    "    return loss,train_acc,test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a1bc2aa-cb01-4b56-8ae7-5e7051e52700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fine_tuning(net, learning_rate, trainloader, testloader, batch_size=128, num_epochs=20,\n",
    "                      param_group=True):\n",
    "    devices = d2l.try_all_gpus()\n",
    "    loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    if param_group:\n",
    "        params_1x = [param for name, param in net.named_parameters()\n",
    "             if name not in [\"classifier.6.weight\", \"classifier.6.bias\"]]\n",
    "        trainer = torch.optim.SGD([{'params': params_1x},\n",
    "                                   {'params': net.classifier[-1].parameters(),\n",
    "                                    'lr': learning_rate * 10}],\n",
    "                                lr=learning_rate, weight_decay=0.001)\n",
    "    else:\n",
    "        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,\n",
    "                                  weight_decay=0.001)\n",
    "    \n",
    "    loss,train_acc,test_acc = train_model(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "               devices=d2l.try_all_gpus())\n",
    "    return loss,train_acc,test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf5c1f6c-0ddf-4d5c-a11c-c084919571bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_result(learning_rate,batch_size,num_epochs,param_group=True):\n",
    "    loss_ls =[]\n",
    "    train_acc_ls =[]\n",
    "    test_acc_ls = []\n",
    "    for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "\n",
    "        print('Fold {}'.format(fold + 1))\n",
    "\n",
    "        #all test there means validation\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_idx) \n",
    "        test_sampler = SubsetRandomSampler(val_idx)\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "        \n",
    "        net = get_net()\n",
    "\n",
    "        loss,train_acc,test_acc = train_fine_tuning(net, learning_rate, train_loader, test_loader, batch_size, num_epochs,\n",
    "                          param_group=True)\n",
    "\n",
    "        loss_ls.append(loss)\n",
    "        train_acc_ls.append(train_acc)\n",
    "        test_acc_ls.append(test_acc)\n",
    "        print('loss: ',loss)\n",
    "        print('train: ',train_acc)\n",
    "        print('test: ',test_acc)\n",
    "    return loss_ls,train_acc_ls,test_acc_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce31f060-3651-4101-bf6c-87d36c2f34c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "loss:  0.5379621434802851\n",
      "train:  0.8140495867768595\n",
      "test:  0.8112582781456954\n",
      "Fold 2\n",
      "loss:  0.48021607320170756\n",
      "train:  0.8347107438016529\n",
      "test:  0.7814569536423841\n",
      "Fold 3\n",
      "loss:  0.5103147585529927\n",
      "train:  0.8363636363636363\n",
      "test:  0.7847682119205298\n",
      "Fold 4\n",
      "loss:  0.5162010114055035\n",
      "train:  0.8330578512396695\n",
      "test:  0.7947019867549668\n",
      "Fold 5\n",
      "loss:  0.4810855404404569\n",
      "train:  0.8528925619834711\n",
      "test:  0.7748344370860927\n"
     ]
    }
   ],
   "source": [
    "loss_ls,train_acc_ls,test_acc_ls = cv_result(learning_rate = 10e-5,batch_size=32,num_epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a0217c8-45d0-4f39-9261-b252cbddb9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5fold_average_loss: 0.505, 5fold_average_train_accuracy: 0.834, 5fold_average_test_accuracy: 0.789\n"
     ]
    }
   ],
   "source": [
    "loss_avg = sum(loss_ls)/len(loss_ls)\n",
    "train_acc_avg = sum(train_acc_ls)/len(train_acc_ls)\n",
    "test_acc_avg = sum(test_acc_ls)/len(test_acc_ls)\n",
    "print(f'5fold_average_loss: {loss_avg:.3f}, 5fold_average_train_accuracy: '\n",
    "      f'{train_acc_avg:.3f}, 5fold_average_test_accuracy: {test_acc_avg:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a823229a-2acd-46fb-8b06-19b2e0108b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "loss:  0.4057918820499389\n",
      "train:  0.8677685950413223\n",
      "test:  0.7980132450331126\n",
      "Fold 2\n",
      "loss:  0.4512549167822215\n",
      "train:  0.8520661157024794\n",
      "test:  0.7781456953642384\n",
      "Fold 3\n",
      "loss:  0.39370934904114274\n",
      "train:  0.8661157024793389\n",
      "test:  0.7781456953642384\n",
      "Fold 4\n",
      "loss:  0.45796194037130056\n",
      "train:  0.8454545454545455\n",
      "test:  0.7649006622516556\n",
      "Fold 5\n",
      "loss:  nan\n",
      "train:  0.07933884297520662\n",
      "test:  0.10927152317880795\n"
     ]
    }
   ],
   "source": [
    "loss_ls,train_acc_ls,test_acc_ls = cv_result(learning_rate = 5e-4,batch_size=32,num_epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "700e692e-2b69-4cdb-a9d4-966062822c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5fold_average_loss: nan, 5fold_average_train_accuracy: 0.702, 5fold_average_test_accuracy: 0.646\n"
     ]
    }
   ],
   "source": [
    "loss_avg = sum(loss_ls)/len(loss_ls)\n",
    "train_acc_avg = sum(train_acc_ls)/len(train_acc_ls)\n",
    "test_acc_avg = sum(test_acc_ls)/len(test_acc_ls)\n",
    "print(f'5fold_average_loss: {loss_avg:.3f}, 5fold_average_train_accuracy: '\n",
    "      f'{train_acc_avg:.3f}, 5fold_average_test_accuracy: {test_acc_avg:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae4daa34-f28e-479a-a9be-e925c84246aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "loss:  0.4928365847295966\n",
      "train:  0.8297520661157025\n",
      "test:  0.7947019867549668\n",
      "Fold 2\n",
      "loss:  0.5409702096103637\n",
      "train:  0.8140495867768595\n",
      "test:  0.8013245033112583\n",
      "Fold 3\n",
      "loss:  0.47047847164563894\n",
      "train:  0.8487603305785124\n",
      "test:  0.7781456953642384\n",
      "Fold 4\n",
      "loss:  0.4933343773045816\n",
      "train:  0.8446280991735537\n",
      "test:  0.7715231788079471\n",
      "Fold 5\n",
      "loss:  0.4761706029088044\n",
      "train:  0.8314049586776859\n",
      "test:  0.7384105960264901\n"
     ]
    }
   ],
   "source": [
    "loss_ls,train_acc_ls,test_acc_ls = cv_result(learning_rate = 10e-5,batch_size=64,num_epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "872b3226-8753-43c3-ac51-9e39a13c0908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5fold_average_loss: 0.495, 5fold_average_train_accuracy: 0.834, 5fold_average_test_accuracy: 0.777\n"
     ]
    }
   ],
   "source": [
    "loss_avg = sum(loss_ls)/len(loss_ls)\n",
    "train_acc_avg = sum(train_acc_ls)/len(train_acc_ls)\n",
    "test_acc_avg = sum(test_acc_ls)/len(test_acc_ls)\n",
    "print(f'5fold_average_loss: {loss_avg:.3f}, 5fold_average_train_accuracy: '\n",
    "      f'{train_acc_avg:.3f}, 5fold_average_test_accuracy: {test_acc_avg:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d92b157-eb14-4601-88f9-7e860ba60a99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
