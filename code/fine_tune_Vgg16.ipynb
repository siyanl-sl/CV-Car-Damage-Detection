{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fede505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef2d653",
   "metadata": {},
   "source": [
    "train 1210\n",
    "test 302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c57540cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msba7028gpu\\anaconda3\\envs\\d2l\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\msba7028gpu\\anaconda3\\envs\\d2l\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27761e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(100)\n",
    "new_data_dir = r\"C:/Users/msba7028gpu/Downloads/train_valid_test\"   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22b1449",
   "metadata": {},
   "source": [
    "# image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f870e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(100)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.08, 1.0),  \n",
    "                                 ratio=(3.0/4.0, 4.0/3.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # randomly change brightness, contrast, saturation and hue\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    #transforms.Resize(256),\n",
    "    #transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf535a",
   "metadata": {},
   "source": [
    "# data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7603122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = torchvision.datasets.ImageFolder(root=os.path.join(new_data_dir, 'train'),\n",
    "                                            transform=transform_train)\n",
    "test_ds = torchvision.datasets.ImageFolder(root=os.path.join(new_data_dir, 'test'),\n",
    "                                            transform=transform_test)\n",
    "batch_size = 32   #batch size can change\n",
    "train_iter = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)  # shuffle=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8f62d",
   "metadata": {},
   "source": [
    "# k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5fc2f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "dataset = train_ds\n",
    "\n",
    "num_epochs=30\n",
    "batch_size=32\n",
    "#k=5\n",
    "#splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "#foldperf={}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35dc8eb",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67028e55-bff4-4577-8d11-051ec5f581be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net():\n",
    "    net = torchvision.models.vgg16(pretrained=True)\n",
    "    net.classifier[-1] = nn.Linear(in_features=4096, out_features=8)\n",
    "    nn.init.xavier_uniform_(net.classifier[-1].weight)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49cef49",
   "metadata": {},
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b522562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch_ch13(net, X, y, loss, trainer, devices):\n",
    "    \"\"\"Train for a minibatch with mutiple GPUs (defined in Chapter 13).\"\"\"\n",
    "    if isinstance(X, list):\n",
    "        # Required for BERT fine-tuning (to be covered later)\n",
    "        X = [x.to(devices[0]) for x in X]\n",
    "    else:\n",
    "        X = X.to(devices[0])\n",
    "    y = y.to(devices[0])\n",
    "    net.train()\n",
    "    trainer.zero_grad()\n",
    "    pred = net(X)\n",
    "    l = loss(pred, y)\n",
    "    l.sum().backward()\n",
    "    trainer.step()\n",
    "    train_loss_sum = l.sum()\n",
    "    train_acc_sum = d2l.accuracy(pred, y)\n",
    "    return train_loss_sum, train_acc_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbcf4175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "def train_model(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "               devices=d2l.try_all_gpus()):\n",
    "    \"\"\"Train a model with mutiple GPUs (defined in Chapter 13).\"\"\"\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Sum of training loss, sum of training accuracy, no. of examples,\n",
    "        # no. of predictions\n",
    "        metric = d2l.Accumulator(4)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            l, acc = train_batch_ch13(\n",
    "                net, features, labels, loss, trainer, devices)\n",
    "            metric.add(l, acc, labels.shape[0], labels.numel())\n",
    "            timer.stop()\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "    \n",
    "    loss = metric[0] / metric[2]\n",
    "    train_acc = metric[1] / metric[3]\n",
    "\n",
    "    return loss,train_acc,test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55aaaba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fine_tuning(net, learning_rate, trainloader, testloader, batch_size=32, num_epochs=30,\n",
    "                      param_group=True):\n",
    "    devices = d2l.try_all_gpus()\n",
    "    loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    if param_group:\n",
    "        params_1x = [param for name, param in net.named_parameters()\n",
    "             if name not in [\"classifier.6.weight\", \"classifier.6.bias\"]]\n",
    "        trainer = torch.optim.SGD([{'params': params_1x},\n",
    "                                   {'params': net.classifier[-1].parameters(),\n",
    "                                    'lr': learning_rate * 10}],\n",
    "                                lr=learning_rate, weight_decay=0.01)\n",
    "    else:\n",
    "        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,\n",
    "                                  weight_decay=0.01)\n",
    "    \n",
    "    loss,train_acc,test_acc = train_model(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "               devices=d2l.try_all_gpus())\n",
    "    return loss,train_acc,test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a602715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_result(learning_rate,batch_size,num_epochs,param_group=True):\n",
    "    loss_ls =[]\n",
    "    train_acc_ls =[]\n",
    "    test_acc_ls = []\n",
    "    for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "\n",
    "        print('Fold {}'.format(fold + 1))\n",
    "\n",
    "        #all test there means validation\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_idx) \n",
    "        test_sampler = SubsetRandomSampler(val_idx)\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "        \n",
    "        net = get_net()\n",
    "\n",
    "        loss,train_acc,test_acc = train_fine_tuning(net, learning_rate, train_loader, test_loader, batch_size=32, num_epochs=30,\n",
    "                          param_group=True)\n",
    "\n",
    "        loss_ls.append(loss)\n",
    "        train_acc_ls.append(train_acc)\n",
    "        test_acc_ls.append(test_acc)\n",
    "        print('loss: ',loss)\n",
    "        print('train: ',train_acc)\n",
    "        print('test: ',test_acc)\n",
    "    return loss_ls,train_acc_ls,test_acc_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e4769d2-39ee-41d0-93c2-24c07566d372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "loss:  0.5136792986846167\n",
      "train:  0.8256198347107438\n",
      "test:  0.7906976744186046\n",
      "Fold 2\n",
      "loss:  0.5303957671173348\n",
      "train:  0.8231404958677686\n",
      "test:  0.8039867109634552\n",
      "Fold 3\n",
      "loss:  0.5440735903653231\n",
      "train:  0.8066115702479338\n",
      "test:  0.7940199335548173\n",
      "Fold 4\n",
      "loss:  0.5095627729557762\n",
      "train:  0.8214876033057851\n",
      "test:  0.7906976744186046\n",
      "Fold 5\n",
      "loss:  0.5344766656229318\n",
      "train:  0.8289256198347107\n",
      "test:  0.7807308970099668\n",
      "5fold_average_loss: 0.526, 5fold_average_train_accuracy: 0.821, 5fold_average_test_accuracy: 0.792\n"
     ]
    }
   ],
   "source": [
    "#the first combination of hyperparameters\n",
    "loss_ls,train_acc_ls,test_acc_ls = cv_result(0.0001,32,25,param_group=True)\n",
    "loss_avg = sum(loss_ls)/len(loss_ls)\n",
    "train_acc_avg = sum(train_acc_ls)/len(train_acc_ls)\n",
    "test_acc_avg = sum(test_acc_ls)/len(test_acc_ls)\n",
    "print(f'5fold_average_loss: {loss_avg:.3f}, 5fold_average_train_accuracy: '\n",
    "      f'{train_acc_avg:.3f}, 5fold_average_test_accuracy: {test_acc_avg:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9ead4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "loss:  0.529166266543806\n",
      "train:  0.8115702479338843\n",
      "test:  0.7873754152823921\n",
      "Fold 2\n",
      "loss:  0.5246661320205562\n",
      "train:  0.8231404958677686\n",
      "test:  0.7973421926910299\n",
      "Fold 3\n",
      "loss:  0.5080617794320603\n",
      "train:  0.8297520661157025\n",
      "test:  0.7973421926910299\n",
      "Fold 4\n",
      "loss:  0.517758906183164\n",
      "train:  0.8330578512396695\n",
      "test:  0.7940199335548173\n",
      "Fold 5\n",
      "loss:  0.4994344439388307\n",
      "train:  0.8314049586776859\n",
      "test:  0.7641196013289037\n",
      "5fold_average_loss: 0.516, 5fold_average_train_accuracy: 0.826, 5fold_average_test_accuracy: 0.788\n"
     ]
    }
   ],
   "source": [
    "#the second combination of hyperparameters\n",
    "loss_ls,train_acc_ls,test_acc_ls = cv_result(0.0001,64,25,param_group=True)\n",
    "loss_avg = sum(loss_ls)/len(loss_ls)\n",
    "train_acc_avg = sum(train_acc_ls)/len(train_acc_ls)\n",
    "test_acc_avg = sum(test_acc_ls)/len(test_acc_ls)\n",
    "print(f'5fold_average_loss: {loss_avg:.3f}, 5fold_average_train_accuracy: '\n",
    "      f'{train_acc_avg:.3f}, 5fold_average_test_accuracy: {test_acc_avg:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aac24df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "loss:  0.46655266580502847\n",
      "train:  0.8512396694214877\n",
      "test:  0.7807308970099668\n",
      "Fold 2\n",
      "loss:  0.4650608433179619\n",
      "train:  0.8471074380165289\n",
      "test:  0.8205980066445183\n",
      "Fold 3\n",
      "loss:  0.47752330441120244\n",
      "train:  0.8347107438016529\n",
      "test:  0.7674418604651163\n",
      "Fold 4\n",
      "loss:  0.45781230532433376\n",
      "train:  0.8413223140495868\n",
      "test:  0.7840531561461794\n",
      "Fold 5\n",
      "loss:  0.5005391215489916\n",
      "train:  0.8330578512396695\n",
      "test:  0.813953488372093\n",
      "5fold_average_loss: 0.473, 5fold_average_train_accuracy: 0.841, 5fold_average_test_accuracy: 0.793\n"
     ]
    }
   ],
   "source": [
    "#the third combination of hyperparameters\n",
    "loss_ls,train_acc_ls,test_acc_ls = cv_result(0.0001,32,30,param_group=True)\n",
    "loss_avg = sum(loss_ls)/len(loss_ls)\n",
    "train_acc_avg = sum(train_acc_ls)/len(train_acc_ls)\n",
    "test_acc_avg = sum(test_acc_ls)/len(test_acc_ls)\n",
    "print(f'5fold_average_loss: {loss_avg:.3f}, 5fold_average_train_accuracy: '\n",
    "      f'{train_acc_avg:.3f}, 5fold_average_test_accuracy: {test_acc_avg:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
