{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fede505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef2d653",
   "metadata": {},
   "source": [
    "train 1210\n",
    "test 302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c57540cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msba7028gpu\\anaconda3\\envs\\d2l\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\msba7028gpu\\anaconda3\\envs\\d2l\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27761e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(100)\n",
    "new_data_dir = r\"C:/Users/msba7028gpu/Downloads/train_valid_test\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22b1449",
   "metadata": {},
   "source": [
    "# image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f870e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(100)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.08, 1.0),  \n",
    "                                 ratio=(3.0/4.0, 4.0/3.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # randomly change brightness, contrast, saturation and hue\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    #transforms.Resize(256),\n",
    "    #transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf535a",
   "metadata": {},
   "source": [
    "# data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7603122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = torchvision.datasets.ImageFolder(root=os.path.join(new_data_dir, 'train'),\n",
    "                                            transform=transform_train)\n",
    "test_ds = torchvision.datasets.ImageFolder(root=os.path.join(new_data_dir, 'test'),\n",
    "                                            transform=transform_test)\n",
    "batch_size = 32   #batch size can change\n",
    "train_iter = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)  # shuffle=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8f62d",
   "metadata": {},
   "source": [
    "# k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5fc2f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#dataset = ConcatDataset([train_ds, test_ds])\n",
    "dataset = train_ds\n",
    "\n",
    "num_epochs=60\n",
    "batch_size=32\n",
    "k=5\n",
    "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "foldperf={}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35dc8eb",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67028e55-bff4-4577-8d11-051ec5f581be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net():\n",
    "    net = nn.Sequential(nn.Flatten(),\n",
    "                    nn.Linear(150528, 3000),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(3000, 1000),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(1000, 500),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(500, 50),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(50, 8))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49cef49",
   "metadata": {},
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b522562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch_ch13(net, X, y, loss, trainer, devices):\n",
    "    \"\"\"Train for a minibatch with mutiple GPUs (defined in Chapter 13).\"\"\"\n",
    "    if isinstance(X, list):\n",
    "        X = [x.to(devices[0]) for x in X]\n",
    "    else:\n",
    "        X = X.to(devices[0])\n",
    "    y = y.to(devices[0])\n",
    "    net.train()\n",
    "    trainer.zero_grad()\n",
    "    pred = net(X)\n",
    "    l = loss(pred, y)\n",
    "    l.sum().backward()\n",
    "    trainer.step()\n",
    "    train_loss_sum = l.sum()\n",
    "    train_acc_sum = d2l.accuracy(pred, y)\n",
    "    return train_loss_sum, train_acc_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbcf4175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "def train_model(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "               devices=d2l.try_all_gpus()):\n",
    "    \"\"\"Train a model with mutiple GPUs (defined in Chapter 13).\"\"\"\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Sum of training loss, sum of training accuracy, no. of examples,\n",
    "        # no. of predictions\n",
    "        metric = d2l.Accumulator(4)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            l, acc = train_batch_ch13(\n",
    "                net, features, labels, loss, trainer, devices)\n",
    "            metric.add(l, acc, labels.shape[0], labels.numel())\n",
    "            timer.stop()\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "    \n",
    "    loss = metric[0] / metric[2]\n",
    "    train_acc = metric[1] / metric[3]\n",
    "\n",
    "    return loss,train_acc,test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55aaaba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_fine_tuning(net, learning_rate, trainloader, testloader, batch_siz, num_epochs,\n",
    "                      param_group=False):\n",
    "    devices = d2l.try_all_gpus()\n",
    "    loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    trainer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    loss,train_acc,test_acc = train_model(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "               devices=d2l.try_all_gpus())\n",
    "    return loss,train_acc,test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a602715",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cv_result(learning_rate,batch_size,num_epochs,param_group=False):\n",
    "    loss_ls =[]\n",
    "    train_acc_ls =[]\n",
    "    test_acc_ls = []\n",
    "    for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "\n",
    "        print('Fold {}'.format(fold + 1))\n",
    "\n",
    "        #all test there means validation\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_idx) \n",
    "        test_sampler = SubsetRandomSampler(val_idx)\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "        \n",
    "        net = get_net()\n",
    "\n",
    "        loss,train_acc,test_acc = train_fine_tuning(net, learning_rate, train_loader, test_loader, batch_size, num_epochs,\n",
    "                          param_group=False)\n",
    "\n",
    "        loss_ls.append(loss)\n",
    "        train_acc_ls.append(train_acc)\n",
    "        test_acc_ls.append(test_acc)\n",
    "        print('loss: ',loss)\n",
    "        print('train: ',train_acc)\n",
    "        print('test: ',test_acc)\n",
    "    return loss_ls,train_acc_ls,test_acc_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aac24df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "loss:  1.8283056527129875\n",
      "train:  0.36446280991735536\n",
      "test:  0.3576158940397351\n",
      "Fold 2\n",
      "loss:  1.8268317892531718\n",
      "train:  0.36446280991735536\n",
      "test:  0.3576158940397351\n",
      "Fold 3\n",
      "loss:  1.8400364048224835\n",
      "train:  0.36446280991735536\n",
      "test:  0.3576158940397351\n",
      "Fold 4\n",
      "loss:  1.8364410967866251\n",
      "train:  0.36446280991735536\n",
      "test:  0.3576158940397351\n",
      "Fold 5\n",
      "loss:  1.847793352111312\n",
      "train:  0.36446280991735536\n",
      "test:  0.3576158940397351\n"
     ]
    }
   ],
   "source": [
    "loss_ls,train_acc_ls,test_acc_ls = cv_result(5e-4,32,20,param_group=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be6ac078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5fold_average_loss: 1.836, 5fold_average_train_accuracy: 0.364, 5fold_average_test_accuracy: 0.358\n"
     ]
    }
   ],
   "source": [
    "loss_avg = sum(loss_ls)/len(loss_ls)\n",
    "train_acc_avg = sum(train_acc_ls)/len(train_acc_ls)\n",
    "test_acc_avg = sum(test_acc_ls)/len(test_acc_ls)\n",
    "print(f'5fold_average_loss: {loss_avg:.3f}, 5fold_average_train_accuracy: '\n",
    "      f'{train_acc_avg:.3f}, 5fold_average_test_accuracy: {test_acc_avg:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ff4a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net():\n",
    "    net = nn.Sequential(nn.Flatten(),\n",
    "                    nn.Linear(150528,3000),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(3000, 100),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(100, 8))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6cf3f91-6c4d-4dff-880a-98cf0cd4298e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "loss:  1.833201532915604\n",
      "train:  0.3652892561983471\n",
      "test:  0.3576158940397351\n",
      "Fold 2\n",
      "loss:  1.835037039134128\n",
      "train:  0.36446280991735536\n",
      "test:  0.3576158940397351\n",
      "Fold 3\n",
      "loss:  1.8378856721988395\n",
      "train:  0.3652892561983471\n",
      "test:  0.3576158940397351\n",
      "Fold 4\n",
      "loss:  1.8292326367591039\n",
      "train:  0.36446280991735536\n",
      "test:  0.3576158940397351\n",
      "Fold 5\n",
      "loss:  1.8274923308821749\n",
      "train:  0.36363636363636365\n",
      "test:  0.3576158940397351\n"
     ]
    }
   ],
   "source": [
    "loss_ls,train_acc_ls,test_acc_ls = cv_result(5e-4,32,20,param_group=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81e2e71b-5baf-4ef0-b47b-3856d0e05773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5fold_average_loss: 1.833, 5fold_average_train_accuracy: 0.365, 5fold_average_test_accuracy: 0.358\n"
     ]
    }
   ],
   "source": [
    "loss_avg = sum(loss_ls)/len(loss_ls)\n",
    "train_acc_avg = sum(train_acc_ls)/len(train_acc_ls)\n",
    "test_acc_avg = sum(test_acc_ls)/len(test_acc_ls)\n",
    "print(f'5fold_average_loss: {loss_avg:.3f}, 5fold_average_train_accuracy: '\n",
    "      f'{train_acc_avg:.3f}, 5fold_average_test_accuracy: {test_acc_avg:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67373c6e-cb81-4b0e-85df-1092a571e53e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
